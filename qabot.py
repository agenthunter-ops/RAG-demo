from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader
from langchain.schema import Document
from langchain.prompts import PromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
import os
import streamlit as st
import asyncio

# Fix for event loop issues in Streamlit
import nest_asyncio
nest_asyncio.apply()

# You can use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')

# Set your Google Gemini API key
# Note: Replace this with your actual Gemini API key from https://makersuite.google.com/app/apikey
os.environ["GOOGLE_API_KEY"] = "AIzaSyD9DtgmNZFq9BK1R48I54xzF6ftvuIfOBU"

# Initialize the Gemini model for QA
llm = None
embeddings = None

def initialize_models():
    global llm, embeddings
    if llm is None:
        llm = ChatGoogleGenerativeAI(
            model="gemini-1.5-flash",  # Updated to current available model
            temperature=0.1,
            max_output_tokens=512
        )
    if embeddings is None:
        embeddings = GoogleGenerativeAIEmbeddings(
            model="models/embedding-001"
        )
    return llm, embeddings

## Document loader
def document_loader(file):
    loader = PyPDFLoader(file.name)
    loaded_document = loader.load()  # Load the PDF document , But does not split it into chunks
    return loaded_document

## Text splitter
def text_splitter(data):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=100,
        length_function=len,
    )
    chunks = text_splitter.split_documents(data)
    return chunks

## Embedding model
def get_embeddings():
    # Initialize models if not already done
    _, embeddings = initialize_models()
    return embeddings

## Create vector store
def create_vectorstore(chunks):
    # Create vector store from the chunks using FAISS
    # FAISS is more efficient for larger datasets and faster similarity search
    vectorstore = FAISS.from_documents(
        documents=chunks,
        embedding=get_embeddings()
    )
    return vectorstore

## Create QA chain
def create_qa_chain(vectorstore):
    # Initialize models if not already done
    llm, _ = initialize_models()
    
    # Create a prompt template for QA
    template = """Use the following pieces of context to answer the question at the end. 
    If you don't know the answer, just say that you don't know, don't try to make up an answer.

    Context: {context}

    Question: {question}
    Answer:"""
    
    prompt = PromptTemplate(
        template=template,
        input_variables=["context", "question"]
    )
    
    # Create the retriever
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
    
    # Create the QA chain using LCEL (LangChain Expression Language)
    def format_docs(docs):
        return "\n\n".join([doc.page_content for doc in docs]) # Format documents for the prompt
    
    qa_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()} 
        | prompt
        | llm
        | StrOutputParser()
    ) ## This is the LangChain Expression Language (LCEL) format for chaining operations
    
    return qa_chain

## Process the query using the QA chain
def process_query(file, query):
    try:
        if not file:
            return "Please upload a PDF file first."
        
        if not query.strip():
            return "Please enter a question."
        
        # Load document
        docs = document_loader(file)
        
        # Split into chunks
        chunks = text_splitter(docs)
        
        if not chunks:
            return "Could not extract text from the PDF. Please check if it's a valid PDF file."
        
        # Create vector store
        vectorstore = create_vectorstore(chunks)
        
        # Create QA chain
        qa_chain = create_qa_chain(vectorstore)
        
        # Process the query
        result = qa_chain.invoke(query)
        
        return result
    
    except Exception as e:
        return f"Error processing your request: {str(e)}"

## Streamlit interface
def main():
    st.set_page_config(page_title="PDF Question Answering Bot", page_icon="ðŸ“š")
    
    st.title("ðŸ“š PDF Question Answering Bot")
    st.markdown("Upload a PDF document and ask questions about its content!")
    
    # File upload
    uploaded_file = st.file_uploader("Choose a PDF file", type="pdf")
    
    # Query input
    query = st.text_input("Ask a question about your document:")
    
    # Submit button
    if st.button("Submit"):
        if uploaded_file is not None and query:
            with st.spinner("Processing your question..."):
                # Save uploaded file temporarily
                with open("temp_pdf.pdf", "wb") as f:
                    f.write(uploaded_file.getbuffer())
                
                # Create a temporary file object for processing
                class TempFile:
                    def __init__(self, name):
                        self.name = name
                
                temp_file = TempFile("temp_pdf.pdf")
                
                # Process the query
                result = process_query(temp_file, query)
                
                # Display result
                st.success("Answer:")
                st.write(result)
                
                # Clean up temporary file
                if os.path.exists("temp_pdf.pdf"):
                    os.remove("temp_pdf.pdf")
        else:
            if not uploaded_file:
                st.warning("Please upload a PDF file first.")
            if not query:
                st.warning("Please enter a question.")

# Run the Streamlit app
if __name__ == "__main__":
    main()